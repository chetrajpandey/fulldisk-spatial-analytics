{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410bafd4-9cbd-4a7b-a3a2-d13ba68307f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from model import Custom_AlexNet, Custom_VGG16, Custom_ResNet34\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.nn.functional import softmax\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", Warning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "# from evaluation import sklearn_Compatible_preds_and_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddf6202-fe0e-40ec-ac53-44d3c61631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyJP2Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        hmi = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(hmi)\n",
    "            \n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        \n",
    "        return (image, y_label, img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6a7a4c-04ed-4951-b2c9-5508db8895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "im_size = 512\n",
    "datapath = '/scratch/cpandey1/hmi_jpgs_512/'\n",
    "partition1_path = '../data_labeling/data_labels/Fold1_val.csv'\n",
    "partition2_path = '../data_labeling/data_labels/Fold2_val.csv'\n",
    "partition3_path = '../data_labeling/data_labels/Fold3_val.csv'\n",
    "partition4_path = '../data_labeling/data_labels/Fold4_val.csv'\n",
    "\n",
    "\n",
    "transformations = Compose([\n",
    "    Resize(im_size),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "part1 = MyJP2Dataset(csv_file = partition1_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)\n",
    "part2 = MyJP2Dataset(csv_file = partition2_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)\n",
    "part3 = MyJP2Dataset(csv_file = partition3_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)\n",
    "part4 = MyJP2Dataset(csv_file = partition4_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6f87a6-ec32-4284-a101-844d1bb72fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_loader = DataLoader(dataset=part1, batch_size=48, num_workers=4, shuffle=False)\n",
    "part2_loader = DataLoader(dataset=part2, batch_size=48, num_workers=4, shuffle=False)\n",
    "part3_loader = DataLoader(dataset=part3, batch_size=48, num_workers=4, shuffle=False)\n",
    "part4_loader = DataLoader(dataset=part4, batch_size=48, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee167eb3-1e70-4cdf-9ca5-22dcd43f5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "#Saved Models Path\n",
    "model_PATH1_alex = '../../tsas_models/Model_alex_Epoch_16_fold1.pth'\n",
    "model_PATH2_alex = '../../tsas_models/Model_alex_Epoch_12_fold2.pth'\n",
    "model_PATH3_alex = '../../tsas_models/Model_alex_Epoch_14_fold3.pth'\n",
    "model_PATH4_alex = '../../tsas_models/Model_alex_Epoch_16_fold4.pth'\n",
    "\n",
    "\n",
    "model_PATH1_vgg = '../../tsas_models/Model_vgg_Epoch_40_fold1.pth'\n",
    "model_PATH2_vgg = '../../tsas_models/Model_vgg_Epoch_21_fold2.pth'\n",
    "model_PATH3_vgg = '../../tsas_models/Model_vgg_Epoch_17_fold3.pth'\n",
    "model_PATH4_vgg = '../../tsas_models/Model_vgg_Epoch_18_fold4.pth'\n",
    "\n",
    "\n",
    "model_PATH1_resnet = '../../tsas_models/Model_resnet_Epoch_27_fold1.pth'\n",
    "model_PATH2_resnet = '../../tsas_models/Model_resnet_Epoch_12_fold2.pth'\n",
    "model_PATH3_resnet = '../../tsas_models/Model_resnet_Epoch_30_fold3.pth'\n",
    "model_PATH4_resnet = '../../tsas_models/Model_resnet_Epoch_22_fold4.pth'\n",
    "\n",
    "#Loading Models Weight\n",
    "weights1_alex = torch.load(model_PATH1_alex)\n",
    "weights2_alex = torch.load(model_PATH2_alex)\n",
    "weights3_alex = torch.load(model_PATH3_alex)\n",
    "weights4_alex = torch.load(model_PATH4_alex)\n",
    "\n",
    "\n",
    "weights1_vgg = torch.load(model_PATH1_vgg)\n",
    "weights2_vgg = torch.load(model_PATH2_vgg)\n",
    "weights3_vgg = torch.load(model_PATH3_vgg)\n",
    "weights4_vgg = torch.load(model_PATH4_vgg)\n",
    "\n",
    "weights1_resnet = torch.load(model_PATH1_resnet)\n",
    "weights2_resnet = torch.load(model_PATH2_resnet)\n",
    "weights3_resnet = torch.load(model_PATH3_resnet)\n",
    "weights4_resnet = torch.load(model_PATH4_resnet)\n",
    "\n",
    "#Defining Models Arch.\n",
    "test_model_alex = Custom_AlexNet(ipt_size=(512, 512), train=False).to(device)\n",
    "test_model_vgg = Custom_VGG16(ipt_size=(512, 512), train=False).to(device)\n",
    "test_model_resnet = Custom_ResNet34(ipt_size=(512, 512), train=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6019d03c-aab0-43d1-8c6a-7766bdd92bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_Compatible_preds_and_targets(model_prediction_list, model_target_list, model_path_list):\n",
    "    y_pred_list = []\n",
    "    preds = []\n",
    "    target_list = []\n",
    "    tgts = []\n",
    "    path_list = []\n",
    "    path = []\n",
    "    y_pred_list = [a.squeeze().tolist() for a in model_prediction_list]\n",
    "    preds = [item for sublist in y_pred_list for item in sublist]\n",
    "    target_list = [a.squeeze().tolist() for a in model_target_list]\n",
    "    tgts = [item for sublist in target_list for item in sublist]\n",
    "    path_list = [a for a in model_path_list]\n",
    "    path = [item for sublist in path_list for item in sublist]\n",
    "    return preds,tgts, path\n",
    "\n",
    "\n",
    "def accuracy_score(prediction, target):\n",
    "    TN, FP, FN, TP = confusion_matrix(target, prediction).ravel()\n",
    "    print(\"TP: \", TP, \"FP: \", FP, \"TN: \", TN, \"FN: \", FN)\n",
    "    #TSS Computation also known as \"recall\"\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    TSS = tp_rate - fp_rate\n",
    "    \n",
    "    #HSS2 Computation\n",
    "    N = TN + FP\n",
    "    P = TP + FN\n",
    "    HSS = (2 * (TP * TN - FN * FP)) / float((P * (FN + TN) + (TP + FP) * N))\n",
    "\n",
    "    return TSS, HSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e3e4b5-c830-4606-b70d-eaa5dbfb1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(checkpoint, test_model, test_loader, desc ):\n",
    "    test_target_list=[]\n",
    "    test_prediction_list=[]\n",
    "    test_path_list = []\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    print('***********************', desc, '*************************')\n",
    "    with torch.no_grad():\n",
    "        for d, t, path in test_loader:\n",
    "            # Get data to cuda if possible\n",
    "            d = d.to(device=device)\n",
    "            t = t.to(device=device)\n",
    "    #         pa = path.to(device=device)\n",
    "            test_target_list.append(t)\n",
    "            test_path_list.append(list(path))\n",
    "    #         print(list(path))\n",
    "            # forward pass\n",
    "            s = test_model(d)\n",
    "            #print(\"scores\", s)\n",
    "\n",
    "            # validation batch loss and accuracy\n",
    "    #         l = criterion(s, t)\n",
    "            p = softmax(s,dim=1)\n",
    "    #         print(p[:,1])\n",
    "            test_prediction_list.append(p[:,1])\n",
    "            # accumulating the val_loss and accuracy\n",
    "    #         val_loss += l.item()\n",
    "            #val_acc += acc.item()\n",
    "            del d,t,s,p\n",
    "    a, b, c = sklearn_Compatible_preds_and_targets(test_prediction_list, test_target_list, test_path_list)\n",
    "    preds = [int(i >=0.5) for i in a]\n",
    "    print(accuracy_score(preds, b))\n",
    "    prob_list = pd.DataFrame(\n",
    "    {'timestamp': c,\n",
    "     'flare_prob': a,\n",
    "     'target': b\n",
    "    })\n",
    "\n",
    "    print(prob_list['target'].value_counts())\n",
    "#     prob_list['timestamp'] = prob_list['timestamp'].apply(lambda row: row[35:-4])\n",
    "#     prob_list['timestamp'] = pd.to_datetime(prob_list['timestamp'], format='%Y.%m.%d_%H.%M.%S')\n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d33563-d191-4557-93ac-b9664efffc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** ALEXNET *************************\n",
      "*********************** Fold-1 Results *************************\n",
      "TP:  1729 FP:  2225 TN:  10229 FN:  605\n",
      "(0.5621308867360248, 0.43847814062565577)\n",
      "0    12454\n",
      "1     2334\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-2 Results *************************\n",
      "TP:  1075 FP:  2298 TN:  11557 FN:  537\n",
      "(0.5010127490232495, 0.3379136966876905)\n",
      "0    13855\n",
      "1     1612\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-3 Results *************************\n",
      "TP:  1660 FP:  3291 TN:  11017 FN:  704\n",
      "(0.47218847903531064, 0.3241356703323698)\n",
      "0    14308\n",
      "1     2364\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-4 Results *************************\n",
      "TP:  2209 FP:  3549 TN:  10483 FN:  481\n",
      "(0.5682676982616472, 0.3889743690364631)\n",
      "0    14032\n",
      "1     2690\n",
      "Name: target, dtype: int64\n",
      "*********************** VGG16 *************************\n",
      "*********************** Fold-1 Results *************************\n",
      "TP:  1704 FP:  2067 TN:  10387 FN:  630\n",
      "(0.5641063483800334, 0.45123154124609755)\n",
      "0    12454\n",
      "1     2334\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-2 Results *************************\n",
      "TP:  1233 FP:  3401 TN:  10454 FN:  379\n",
      "(0.51941738835314, 0.28410060452178976)\n",
      "0    13855\n",
      "1     1612\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-3 Results *************************\n",
      "TP:  1409 FP:  3089 TN:  11219 FN:  955\n",
      "(0.380130482065575, 0.2761011124291257)\n",
      "0    14308\n",
      "1     2364\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-4 Results *************************\n",
      "TP:  2125 FP:  3236 TN:  10796 FN:  565\n",
      "(0.5593470898170089, 0.3991649531705525)\n",
      "0    14032\n",
      "1     2690\n",
      "Name: target, dtype: int64\n",
      "*********************** ResNet34 *************************\n",
      "*********************** Fold-1 Results *************************\n",
      "TP:  1779 FP:  2145 TN:  10309 FN:  555\n",
      "(0.5899769764558769, 0.4620811943275134)\n",
      "0    12454\n",
      "1     2334\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-2 Results *************************\n",
      "TP:  1257 FP:  3872 TN:  9983 FN:  355\n",
      "(0.5003107781498021, 0.25474601358021987)\n",
      "0    13855\n",
      "1     1612\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-3 Results *************************\n",
      "TP:  1328 FP:  2299 TN:  12009 FN:  1036\n",
      "(0.4010803890431772, 0.3279491054022424)\n",
      "0    14308\n",
      "1     2364\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-4 Results *************************\n",
      "TP:  2160 FP:  3382 TN:  10650 FN:  530\n",
      "(0.561953453179774, 0.39339075840305854)\n",
      "0    14032\n",
      "1     2690\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('*********************** ALEXNET *************************')\n",
    "fold1_alex = predict(weights1_alex, test_model_alex, part1_loader, 'Fold-1 Results')\n",
    "fold2_alex = predict(weights2_alex, test_model_alex, part2_loader, 'Fold-2 Results')\n",
    "fold3_alex = predict(weights3_alex, test_model_alex, part3_loader, 'Fold-3 Results')\n",
    "fold4_alex = predict(weights4_alex, test_model_alex, part4_loader, 'Fold-4 Results')\n",
    "\n",
    "print('*********************** VGG16 *************************')\n",
    "fold1_vgg = predict(weights1_vgg, test_model_vgg, part1_loader, 'Fold-1 Results')\n",
    "fold2_vgg = predict(weights2_vgg, test_model_vgg, part2_loader, 'Fold-2 Results')\n",
    "fold3_vgg = predict(weights3_vgg, test_model_vgg, part3_loader, 'Fold-3 Results')\n",
    "fold4_vgg = predict(weights4_vgg, test_model_vgg, part4_loader, 'Fold-4 Results')\n",
    "\n",
    "print('*********************** ResNet34 *************************')\n",
    "fold1_resnet = predict(weights1_resnet, test_model_resnet, part1_loader, 'Fold-1 Results')\n",
    "fold2_resnet = predict(weights2_resnet, test_model_resnet, part2_loader, 'Fold-2 Results')\n",
    "fold3_resnet = predict(weights3_resnet, test_model_resnet, part3_loader, 'Fold-3 Results')\n",
    "fold4_resnet = predict(weights4_resnet, test_model_resnet, part4_loader, 'Fold-4 Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6008e6c6-cbb5-48af-9607-11dd9e052c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(fold, fold_name, model_name):\n",
    "    fold.to_csv(f'results/{fold_name}_{model_name}.csv', index=False, header=True, columns=['timestamp', 'flare_prob', 'target'])\n",
    "\n",
    "save_results(fold1_alex, 'fold1', 'alex')\n",
    "save_results(fold2_alex, 'fold2', 'alex')\n",
    "save_results(fold3_alex, 'fold3', 'alex')\n",
    "save_results(fold4_alex, 'fold4', 'alex')\n",
    "save_results(fold1_vgg, 'fold1', 'vgg')\n",
    "save_results(fold2_vgg, 'fold2', 'vgg')\n",
    "save_results(fold3_vgg, 'fold3', 'vgg')\n",
    "save_results(fold4_vgg, 'fold4', 'vgg')\n",
    "save_results(fold1_resnet, 'fold1', 'resnet')\n",
    "save_results(fold2_resnet, 'fold2', 'resnet')\n",
    "save_results(fold3_resnet, 'fold3', 'resnet')\n",
    "save_results(fold4_resnet, 'fold4', 'resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbcbb110-9f1b-419f-8a31-f307158cfba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pd.read_csv('M_full_dataset_cleaned_1_hours_with_loc_and_time_new.csv')\n",
    "details['timestamp'] = details['label'].apply(lambda row: row[16:-4])\n",
    "details['timestamp'] = pd.to_datetime(details['timestamp'], format='%Y.%m.%d_%H.%M.%S')\n",
    "details.drop(columns=['label'], inplace=True)\n",
    "\n",
    "def aggregateAndAddLocation(fold_name1,fold_name2,fold_name3,fold_name4, model_name):\n",
    "    fold1_val = pd.read_csv(f'results/{fold_name1}_{model_name}.csv')\n",
    "    fold2_val = pd.read_csv(f'results/{fold_name2}_{model_name}.csv')\n",
    "    fold3_val = pd.read_csv(f'results/{fold_name3}_{model_name}.csv')\n",
    "    fold4_val = pd.read_csv(f'results/{fold_name4}_{model_name}.csv')\n",
    "    total = pd.concat([fold1_val, fold2_val, fold3_val, fold4_val])\n",
    "    total['timestamp'] = total['timestamp'].apply(lambda row: row[47:-4])\n",
    "    total['timestamp'] =  pd.to_datetime(total['timestamp'], format='%Y.%m.%d_%H.%M.%S')\n",
    "    total.reset_index(inplace=True)\n",
    "    df = total.merge(details, how='left', on='timestamp')\n",
    "    return df\n",
    "\n",
    "alex_results = aggregateAndAddLocation('fold1', 'fold2', 'fold3', 'fold4' ,'alex')\n",
    "vgg_results = aggregateAndAddLocation('fold1', 'fold2', 'fold3', 'fold4' ,'vgg')\n",
    "resnet_results = aggregateAndAddLocation('fold1', 'fold2', 'fold3', 'fold4' ,'resnet')\n",
    "# resnet_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f35a81-166d-47ed-b4ae-140dbcf39e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************************AlexNet*****************************\n",
      "*************X Class flares locations***************\n",
      "Total Instances:  880\n",
      "With in Central Locations\n",
      "TP:  614 FN:  54\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  138 FN:  74 \n",
      "\n",
      "*************M Class flares locations***************\n",
      "Total Instances:  8120\n",
      "With in Central Locations\n",
      "TP:  4645 FN:  1185\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  1276 FN:  1014 \n",
      "\n",
      "\n",
      "*****************************VGG16*****************************\n",
      "*************X Class flares locations***************\n",
      "Total Instances:  880\n",
      "With in Central Locations\n",
      "TP:  560 FN:  108\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  165 FN:  47 \n",
      "\n",
      "*************M Class flares locations***************\n",
      "Total Instances:  8120\n",
      "With in Central Locations\n",
      "TP:  4473 FN:  1357\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  1273 FN:  1017 \n",
      "\n",
      "\n",
      "*****************************ResNet*****************************\n",
      "*************X Class flares locations***************\n",
      "Total Instances:  880\n",
      "With in Central Locations\n",
      "TP:  612 FN:  56\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  172 FN:  40 \n",
      "\n",
      "*************M Class flares locations***************\n",
      "Total Instances:  8120\n",
      "With in Central Locations\n",
      "TP:  4449 FN:  1381\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  1291 FN:  999 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def location_analysis(df, flareclass):\n",
    "    X = df.loc[(df.goes_class.str.startswith(flareclass))].copy()\n",
    "    X[[\"x\", \"y\"]] = X[\"fl_location\"].str.strip(r\"[()]\").str.split(\",\", expand=True).astype(str)\n",
    "    X['x'] = pd.to_numeric(X['x']).round(decimals=2).astype(str).replace(r'\\.0$', '', regex=True)\n",
    "    X[[\"x\", \"y\"]] = X[['x', 'y']].astype(float)\n",
    "    pos = X[(X.flare_prob>=0.5)]\n",
    "    neg = X[(X.flare_prob<0.5)]\n",
    "    pos_limb = len(pos.loc[(pos.x<-70) | (pos.x>70)])+len(pos.loc[(pos.y<-70) | (pos.y>70)])\n",
    "    neg_limb = len(neg.loc[(neg.x<-70) | (neg.x>70)])+ len(neg.loc[(neg.y<-70) | (neg.y>70)])\n",
    "    pos_center = len(pos.loc[(pos.y>=-70) & (pos.y<=70) & (pos.x>=-70) & (pos.x<=70)])\n",
    "    neg_center = len(neg.loc[(neg.y>=-70) & (neg.y<=70) & (neg.x>=-70) & (neg.x<=70)])\n",
    "    print(f'*************{flareclass} Class flares locations***************')\n",
    "    print(\"Total Instances: \", len(X))\n",
    "    print('With in Central Locations')\n",
    "    print('TP: ', pos_center, 'FN: ', neg_center)\n",
    "    print('Beyond Central Locations (Limb Locations)')\n",
    "    print('TP: ', pos_limb, 'FN: ', neg_limb, '\\n')\n",
    "    \n",
    "print('\\n*****************************AlexNet*****************************')\n",
    "location_analysis(alex_results.copy(), 'X')\n",
    "location_analysis(alex_results.copy(), 'M')\n",
    "\n",
    "print('\\n*****************************VGG16*****************************')\n",
    "location_analysis(vgg_results.copy(), 'X')\n",
    "location_analysis(vgg_results.copy(), 'M')\n",
    "\n",
    "print('\\n*****************************ResNet*****************************')\n",
    "location_analysis(resnet_results.copy(), 'X')\n",
    "location_analysis(resnet_results.copy(), 'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d905625-4897-4cb8-a0a6-c16a391529ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_filename(df):\n",
    "    cols=['timestamp']\n",
    "    for items in cols:\n",
    "\n",
    "        df[items] = pd.to_datetime(df[items], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        #Renaming label(Date) to this format of file HMI.m2010.05.21_12.00.00 \n",
    "        df[items] = df[items].dt.year.astype(str) + '/' \\\n",
    "            + df[items].dt.month.map(\"{:02}\".format).astype(str) + '/'\\\n",
    "            + df[items].dt.day.map(\"{:02}\".format).astype(str) + '/'+ 'HMI.m'+ df[items].dt.year.astype(str) + '.' \\\n",
    "            + df[items].dt.month.map(\"{:02}\".format).astype(str) + '.'\\\n",
    "            + df[items].dt.day.map(\"{:02}\".format).astype(str) + '_' \\\n",
    "            + df[items].dt.hour.map(\"{:02}\".format).astype(str) + '.'\\\n",
    "            + df[items].dt.minute.map(\"{:02}\".format).astype(str) + '.'\\\n",
    "            + df[items].dt.second.map(\"{:02}\".format).astype(str) + '.jpg'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b24bf18-173a-444f-aff1-751c07e9a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_final(df, modelname):\n",
    "    new_df = df.copy()\n",
    "    df_x = new_df.loc[(new_df.goes_class.str.startswith('X'))]\n",
    "    df_m = new_df.loc[(new_df.goes_class.str.startswith('M'))]\n",
    "    df_c = new_df.loc[(new_df.goes_class.str.startswith('C'))]\n",
    "    cols = ['timestamp', 'flare_prob', 'goes_class', 'fl_location', 'flare_start']\n",
    "    df_x.to_csv(f'results/{modelname}_x_class.csv', index=False, header=True, columns=cols)\n",
    "    df_m.to_csv(f'results/{modelname}_m_class.csv', index=False, header=True, columns=cols)\n",
    "    df_c.to_csv(f'results/{modelname}_c_class.csv', index=False, header=True, columns=cols)\n",
    "save_results_final(alex_results, 'alex')\n",
    "save_results_final(vgg_results, 'vgg')\n",
    "save_results_final(resnet_results, 'resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d3035-a13e-4ffa-8e31-f8002b379524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = date_to_filename(df)\n",
    "df_x = new_df.loc[(new_df.goes_class.str.startswith('X'))]\n",
    "df_m = new_df.loc[(new_df.goes_class.str.startswith('M'))]\n",
    "df_c = new_df.loc[(new_df.goes_class.str.startswith('C')) & (new_df.flare_prob>=0.8)]\n",
    "cols = ['timestamp', 'flare_prob', 'goes_class', 'fl_location', 'flare_start']\n",
    "df_x.to_csv(r'x_class.csv', index=False, header=True, columns=cols)\n",
    "df_m.to_csv(r'm_class.csv', index=False, header=True, columns=cols)\n",
    "df_c.to_csv(r'c_class.csv', index=False, header=True, columns=cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
